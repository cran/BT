<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Gireg Willame" />

<meta name="date" content="2023-08-19" />

<title>Getting started with the BT package</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Getting started with the BT package</h1>
<h4 class="author">Gireg Willame</h4>
<h4 class="date">2023-08-19</h4>


<div id="TOC">
<ul>
<li><a href="#introduction" id="toc-introduction">Introduction</a></li>
<li><a href="#important-modeling-remark" id="toc-important-modeling-remark">Important modeling remark</a></li>
<li><a href="#use-case" id="toc-use-case">Use-case</a>
<ul>
<li><a href="#import-database" id="toc-import-database">Import
database</a></li>
<li><a href="#create-working-datasets" id="toc-create-working-datasets">Create working datasets</a></li>
<li><a href="#boosting-tree-bt" id="toc-boosting-tree-bt">Boosting Tree
(BT)</a>
<ul>
<li><a href="#bt-fit-and-outputs" id="toc-bt-fit-and-outputs"><code>BT</code> fit and outputs</a></li>
<li><a href="#optimal-iterations-number" id="toc-optimal-iterations-number">Optimal iterations number</a></li>
<li><a href="#continue-training" id="toc-continue-training">Continue
training</a></li>
<li><a href="#cross-validation" id="toc-cross-validation">Cross-validation</a></li>
<li><a href="#hyperparameter-optimization" id="toc-hyperparameter-optimization">Hyperparameter
Optimization</a></li>
<li><a href="#relative-influence" id="toc-relative-influence">Relative
influence</a></li>
<li><a href="#prediction" id="toc-prediction">Prediction</a></li>
</ul></li>
<li><a href="#adaptive-boosting-tree-abt" id="toc-adaptive-boosting-tree-abt">Adaptive Boosting Tree (ABT)</a>
<ul>
<li><a href="#hyperparameter-optimization-1" id="toc-hyperparameter-optimization-1">Hyperparameter
Optimization</a></li>
</ul></li>
<li><a href="#miscellaneous" id="toc-miscellaneous">Miscellaneous</a></li>
<li><a href="#models-comparison" id="toc-models-comparison">Models
comparison</a>
<ul>
<li><a href="#deviance" id="toc-deviance">Deviance</a></li>
</ul></li>
</ul></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>The <code>BT</code> package implements (Adaptive) Boosting Tree for
<em>Poisson</em> distributed response variables, using log-link
function. When presented with data, the <code>BT</code> package offers
the user the ability to build predictive models and explore the
influence of different variables on the response, akin to a data mining
or exploration task. The built package is based on the original idea
proposed by D. Hainaut, J. Trufin and M. Denuit. For more theoretical
details, we refer to the following references:</p>
<ul>
<li>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective
Statistical Learning Methods for Actuaries |: GLMs and
Extensions</strong>, <em>Springer Actuarial</em>.</li>
<li>M. Denuit, D. Hainaut and J. Trufin (2020). <strong>Effective
Statistical Learning Methods for Actuaries ||: Tree-Based Methods and
Extensions</strong>, <em>Springer Actuarial</em>.</li>
<li>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective
Statistical Learning Methods for Actuaries |||: Neural Networks and
Extensions</strong>, <em>Springer Actuarial</em>.</li>
<li>M. Denuit, D. Hainaut and J. Trufin (2022). <strong>Response versus
gradient boosting trees, GLMs and neural networks under Tweedie loss and
log-link</strong>, <em>Scandinavian Actuarial Journal 2022,
841-866</em>.</li>
<li>M. Denuit, J. Huyghe and J. Trufin (2022). <strong>Boosting
cost-complexity pruned trees on Tweedie responses: The ABT machine for
insurance ratemaking</strong>. Paper submitted for publication.</li>
<li>M. Denuit, J. Trufin and T. Verdebout (2022). <strong>Boosting on
the responses with Tweedie loss functions</strong>. Paper submitted for
publication.</li>
</ul>
<p>We’ll now show how to use the <code>BT</code> package on classical
machine learning problem. In particular, insurance related model will be
investigated in the following.</p>
<p>Let’s start by importing the <code>BT</code> package.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(BT)</span></code></pre></div>
</div>
<div id="important-modeling-remark" class="section level1">
<h1>Important modeling remark</h1>
<p>As previously mentioned, the <code>BT</code> package is only
available for <em>Poisson</em> distributed response variable <span class="math inline">\(Y\)</span>, in a log-link context.</p>
<p>Using <em>offset</em> in the Poisson framework is often required. In
the insurance modelling for example, the offset allows to take into
account the time exposure-to-risk. It therefore helps to align the asked
premium with respect to the contract duration.</p>
<p>Regarding the <code>BT</code> package, the weighted approach was
favored in place of the offset one. In fact, the two are equivalent
given some adjustments. More precisely, the modeler is allowed to work
either with</p>
<ul>
<li>the observed claim count <span class="math inline">\(Y\)</span> with
offset <span class="math inline">\(ln(d)\)</span> under log link, where
<span class="math inline">\(d\)</span> is the exposure to risk,
generally measured in time units, sometimes in distance traveled or
other meaningful unit depending on the application, or</li>
<li>the observed claim frequency (also called claim rate) <span class="math inline">\(\tilde{Y} = Y/d\)</span> provided the weight <span class="math inline">\(\nu = d\)</span> enters the analysis. In fact, the
distribution of the claim frequency <span class="math inline">\(\tilde{Y}\)</span> still belongs to the Tweedie
family and is called the Poisson rate distribution.</li>
</ul>
<p>We refer to the first book aforementioned (p. 123) for a detailed
proof.</p>
<p>We now focus on the impact of such implementation choice on the
Boosting Tree Algorithm. First of all, let us remind the algorithm in
our Poisson log-link framework. Given a training set</p>
<p><span class="math display">\[\mathcal{D} = \Bigl\{ (d_i, y_i,
\mathbf{x}_i), i \in \mathcal{I} \Bigl\},\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(d_i\)</span> the exposure-to-risk
(e.g. offset term).</li>
<li><span class="math inline">\(y_i\)</span> is the observed count
variable.</li>
<li><span class="math inline">\(\mathbf{x}_i\)</span> the corresponding
features vector.</li>
<li><span class="math inline">\(\mathcal{I}\)</span> corresponds to the
set of all observations,</li>
</ul>
<p>the following steps are performed:</p>
<ol style="list-style-type: decimal">
<li><p>Initialize the score to <span class="math display">\[
\widehat{\text{score}}_0(x) = \mathop{\mathrm{argmin}}_{\beta} \sum_{i
\in \mathcal{I}} L\Bigl(y_i, d_i \exp(\beta) \Bigr).
\]</span></p></li>
<li><p><strong>For</strong> <span class="math inline">\(m = 1\)</span>
to <span class="math inline">\(M\)</span> <strong>do</strong></p>
<p>2.1. Fit a weak learner, regression tree in our context, <span class="math inline">\(T(\mathbf{x}; \widehat{\mathbf{a}_m})\)</span>
with <span class="math display">\[
  \widehat{\mathbf{a}_m} = \mathop{\mathrm{argmin}}_{\mathbf{a}_m}
\sum_{i \in \mathcal{I}} L\biggl(y_i, d_i
\exp\Bigl(\widehat{\text{score}}_{m-1}(\mathbf{x}_i) + T(\mathbf{x}_i;
\mathbf{a}_m)\Bigr)\biggr),
  \]</span> where</p>
<ul>
<li><span class="math inline">\(\mathbf{a}_m\)</span> gathers the
splitting variables and their split values as well as the corresponding
observed averages in the terminal nodes, i.e. describes the built
tree.</li>
<li><span class="math inline">\(L\)</span> is the loss function, defined
as the Poisson deviance in our approach.</li>
</ul>
<p>2.2. Update <span class="math inline">\(\widehat{\text{score}}_m(\mathbf{x}) =
\widehat{\text{score}}_{m-1}(\mathbf{x}) + T(\mathbf{x};
\widehat{\mathbf{a}_m})\)</span>.</p></li>
<li><p>Output <span class="math inline">\(\widehat{\text{score}}(\mathbf{x}) =
\widehat{\text{score}}_M(\mathbf{x}).\)</span></p></li>
</ol>
<p>Suppose that we’re at the <span class="math inline">\(m\)</span>th
boosting iteration, the algorithm then fits a weak learner <span class="math inline">\(T(\mathbf{x}; \widehat{\mathbf{a}_m})\)</span>.
Using the above trick, for a given observation <span class="math inline">\(i\)</span> one can rewrite the optimization step
2.1 as</p>
<p><span class="math display">\[
L\biggl(y_i, d_i \exp\Bigl(\widehat{\text{score}}_{m-1}(\mathbf{x}_i) +
T(\mathbf{x}_i; \mathbf{a}_m)\Bigr)\biggr) =
\nu_i L\biggl(\tilde{y}_i,
\exp\Bigl(\widehat{\text{score}}_{m-1}(\mathbf{x}_i) + T(\mathbf{x}_i;
\mathbf{a}_m)\Bigr)\biggr),
\]</span> where <span class="math inline">\(\nu_i = d_i\)</span> and
<span class="math inline">\(\tilde{y_i} = \frac{y_i}{d_i}\)</span>.
Using the definition of the Poisson deviance <span class="math inline">\(L\)</span>, one can easily rewrite the second term
as:</p>
<p><span class="math display">\[
\nu_i L\biggl(\tilde{y}_i,
\exp\Bigl(\widehat{\text{score}}_{m-1}(\mathbf{x}_i) + T(\mathbf{x}_i;
\mathbf{a}_m)\Bigr)\biggr) = \nu_{mi} L(\tilde{r}_{mi},
\exp(T(\mathbf{x}_i; \mathbf{a}_m))),
\]</span> with <span class="math display">\[
\nu_{mi} = \nu_i \exp\Bigl(\widehat{\text{score}}_{m-1}(\mathbf{x}_i)
\Bigr)
\]</span> and <span class="math display">\[
\tilde{r}_{mi} = \frac{\tilde{y}_i}{\exp \Bigl(
\widehat{\text{score}}_{m-1}(\mathbf{x}_i) \Bigr)}.
\]</span></p>
<p>The <span class="math inline">\(m\)</span>th iteration of the
boosting procedure therefore reduces to build a single weak learner, on
the working training set</p>
<p><span class="math display">\[
\mathcal{D}^{(m)} = \Bigl\{ (\nu_{mi}, \tilde{r}_{mi}, \mathbf{x}_i), i
\in \mathcal{I} \Bigl\},
\]</span></p>
<p>using the Poisson deviance loss and the log-link function. Going
through iterations, the weights are each time updated together with the
responses that are assumed to follow Poisson rate distributions.</p>
</div>
<div id="use-case" class="section level1">
<h1>Use-case</h1>
<p>The goal of this section is to show how the user can work with
<code>BT</code> functions and define an optimal model, according to the
selected criteria. We also underline that this use-case is a toy example
with some limitations such as running-time constraints. However, the
same concepts can easily be extended to real-world problems.</p>
<div id="import-database" class="section level2">
<h2>Import database</h2>
<p>Let us import the simulated database
<code>BT::BT_Simulated_Data</code>. We refer to the specific database
documentation for more details.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>db <span class="ot">&lt;-</span> BT<span class="sc">::</span>BT_Simulated_Data</span></code></pre></div>
<p>One can then have a look at this database.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="fu">str</span>(db)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    50000 obs. of  7 variables:
##  $ Y           : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ Gender      : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 2 2 2 1 2 1 2 1 ...
##  $ Age         : int  28 62 37 56 36 24 52 33 47 51 ...
##  $ Split       : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 2 1 1 1 2 2 1 1 2 ...
##  $ Sport       : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 1 1 2 2 2 1 1 1 1 ...
##  $ ExpoR       : num  0.102 0.712 0.373 0.621 0.106 ...
##  $ Y_normalized: num  0 0 0 0 0 0 0 0 0 0 ...</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="fu">head</span>(db)</span></code></pre></div>
<pre><code>##   Y Gender Age Split Sport      ExpoR Y_normalized
## 1 0 female  28    no   yes 0.10197573            0
## 2 0   male  62   yes    no 0.71179081            0
## 3 0   male  37    no    no 0.37298030            0
## 4 0   male  56    no   yes 0.62134765            0
## 5 0   male  36    no   yes 0.10604182            0
## 6 0 female  24   yes   yes 0.05905927            0</code></pre>
<p>One can also perform a quick summary</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="fu">summary</span>(db)</span></code></pre></div>
<pre><code>##        Y              Gender           Age        Split       Sport      
##  Min.   :0.00000   female:24898   Min.   :18.00   no :24919   no :24995  
##  1st Qu.:0.00000   male  :25102   1st Qu.:29.00   yes:25081   yes:25005  
##  Median :0.00000                  Median :41.00                          
##  Mean   :0.07296                  Mean   :41.47                          
##  3rd Qu.:0.00000                  3rd Qu.:53.00                          
##  Max.   :4.00000                  Max.   :65.00                          
##      ExpoR            Y_normalized    
##  Min.   :0.0000163   Min.   : 0.0000  
##  1st Qu.:0.2494463   1st Qu.: 0.0000  
##  Median :0.4999763   Median : 0.0000  
##  Mean   :0.5002706   Mean   : 0.1443  
##  3rd Qu.:0.7519584   3rd Qu.: 0.0000  
##  Max.   :0.9999931   Max.   :27.0002</code></pre>
<p>We leave potential descriptive analysis to the interested reader but
we note that the global average claim frequency is</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="fu">sum</span>(db<span class="sc">$</span>Y)<span class="sc">/</span><span class="fu">sum</span>(db<span class="sc">$</span>ExpoR)</span></code></pre></div>
<pre><code>## [1] 0.1458411</code></pre>
</div>
<div id="create-working-datasets" class="section level2">
<h2>Create working datasets</h2>
<p>As we’re dealing with machine learning models, a classical approach
consists in splitting the dataset into two parts, namely:</p>
<ul>
<li>A <strong>training set</strong> which will be heavily used to train
the different models and will serve for model selection.</li>
<li>A <strong>testing set</strong> which will be hold off and used at
the end to assess generalization performances.</li>
</ul>
<p>In our example, 80% of the total dataset will be placed in the
training set and the remaining part in the testing set. One can note
that the claims frequency is approximately similar for all the
databases.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">404</span>)</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>trainObs <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">seq</span>(<span class="dv">1</span>, <span class="fu">nrow</span>(db)), <span class="fl">0.8</span> <span class="sc">*</span> <span class="fu">nrow</span>(db))</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>trainSet <span class="ot">&lt;-</span> db[trainObs, ]</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>testSet <span class="ot">&lt;-</span> db[<span class="fu">setdiff</span>(<span class="fu">seq</span>(<span class="dv">1</span>, <span class="fu">nrow</span>(db)), trainObs), ]</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a><span class="fu">sum</span>(trainSet<span class="sc">$</span>Y)<span class="sc">/</span><span class="fu">sum</span>(trainSet<span class="sc">$</span>ExpoR)</span></code></pre></div>
<pre><code>## [1] 0.1457085</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="fu">sum</span>(testSet<span class="sc">$</span>Y)<span class="sc">/</span><span class="fu">sum</span>(testSet<span class="sc">$</span>ExpoR)</span></code></pre></div>
<pre><code>## [1] 0.1463707</code></pre>
</div>
<div id="boosting-tree-bt" class="section level2">
<h2>Boosting Tree (BT)</h2>
<p>The basic idea behind this algorithm consists in building weak
leaners to explain the remaining error, using all the past iterations.
It differs from the Gradient Boosting Methods as we’re here boosting the
ratios (as previously explained) rather than the pseudo-residuals, using
the defined underlying distribution rather than a gaussian approach.</p>
<p>In particular, let us remind that the package does not support
offset. However, a problem reformulation can be used as explained
before.</p>
<p>We want to make profit of all explanatory variables. We then define
the following model formula that will be heavily used.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>formFreq <span class="ot">&lt;-</span> Y_normalized <span class="sc">~</span> Gender <span class="sc">+</span> Age <span class="sc">+</span> Split <span class="sc">+</span> Sport</span></code></pre></div>
<div id="bt-fit-and-outputs" class="section level3">
<h3><code>BT</code> fit and outputs</h3>
<p>We propose to begin this section by looking on a simple example
resulting from a first run. We can then discuss the different available
package’s features.</p>
<p>We refer to the package documentation <code>?BT::BT</code> for more
details about the arguments of this function.</p>
<p>A first <code>BT</code> can be fitted without cross-validation</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>bt0 <span class="ot">&lt;-</span> <span class="fu">BT</span>(<span class="at">formula =</span> formFreq, <span class="at">data =</span> trainSet, <span class="at">tweedie.power =</span> <span class="dv">1</span>, <span class="at">ABT =</span> <span class="cn">FALSE</span>, <span class="at">n.iter =</span> <span class="dv">50</span>,</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>    <span class="at">train.fraction =</span> <span class="fl">0.8</span>, <span class="at">interaction.depth =</span> <span class="dv">3</span>, <span class="at">shrinkage =</span> <span class="fl">0.01</span>, <span class="at">bag.fraction =</span> <span class="fl">0.5</span>,</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>    <span class="at">colsample.bytree =</span> <span class="cn">NULL</span>, <span class="at">keep.data =</span> <span class="cn">TRUE</span>, <span class="at">is.verbose =</span> <span class="cn">FALSE</span>, <span class="at">cv.folds =</span> <span class="dv">1</span>,</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>    <span class="at">folds.id =</span> <span class="cn">NULL</span>, <span class="at">n.cores =</span> <span class="dv">1</span>, <span class="at">weights =</span> ExpoR, <span class="at">seed =</span> <span class="dv">4</span>)</span></code></pre></div>
<p>One can first have a look at the return object. Almost all the
parameters that have been used during the call are stored.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>bt0<span class="sc">$</span>call</span></code></pre></div>
<pre><code>## BT(formula = formFreq, data = trainSet, tweedie.power = 1, ABT = FALSE, 
##     n.iter = 50, train.fraction = 0.8, interaction.depth = 3, 
##     shrinkage = 0.01, bag.fraction = 0.5, colsample.bytree = NULL, 
##     keep.data = TRUE, is.verbose = FALSE, cv.folds = 1, folds.id = NULL, 
##     n.cores = 1, weights = ExpoR, seed = 4)</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>bt0<span class="sc">$</span>distribution</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a>bt0<span class="sc">$</span>BTParams</span></code></pre></div>
<pre><code>## $ABT
## [1] FALSE
## 
## $train.fraction
## [1] 0.8
## 
## $shrinkage
## [1] 0.01
## 
## $interaction.depth
## [1] 3
## 
## $bag.fraction
## [1] 0.5
## 
## $n.iter
## [1] 50
## 
## $colsample.bytree
## NULL
## 
## $tree.control
## $tree.control$minsplit
## [1] 2
## 
## $tree.control$minbucket
## [1] 1
## 
## $tree.control$cp
## [1] -Inf
## 
## $tree.control$maxcompete
## [1] 4
## 
## $tree.control$maxsurrogate
## [1] 5
## 
## $tree.control$usesurrogate
## [1] 2
## 
## $tree.control$surrogatestyle
## [1] 0
## 
## $tree.control$maxdepth
## [1] 3
## 
## $tree.control$xval
## [1] 0
## 
## 
## attr(,&quot;class&quot;)
## [1] &quot;BTParams&quot;</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a>bt0<span class="sc">$</span>keep.data</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a>bt0<span class="sc">$</span>is.verbose</span></code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a>bt0<span class="sc">$</span>seed</span></code></pre></div>
<pre><code>## [1] 4</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="co"># bt0$w / bt0$response / bt0$var.name</span></span></code></pre></div>
<p>A built-in <code>print</code> function is also available. This method
prints some of the already presented values.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a><span class="fu">print</span>(bt0)</span></code></pre></div>
<pre><code>## BT(formula = formFreq, data = trainSet, tweedie.power = 1, ABT = FALSE, 
##     n.iter = 50, train.fraction = 0.8, interaction.depth = 3, 
##     shrinkage = 0.01, bag.fraction = 0.5, colsample.bytree = NULL, 
##     keep.data = TRUE, is.verbose = FALSE, cv.folds = 1, folds.id = NULL, 
##     n.cores = 1, weights = ExpoR, seed = 4)
## A boosting tree model with Tweedie parameter : 1  has been fitted.
##  50 iterations were performed.</code></pre>
<pre><code>## OOB generally underestimates the optimal number of iterations although predictive performance is reasonably competitive.
##             Using cv_folds&gt;1 when calling BT usually results in improved predictive performance.</code></pre>
<pre><code>## The best out-of-bag iteration was 50.
## The best validation-set iteration was 50.
## There were 4 predictors of which 4 had non-zero influence.</code></pre>
<p>One can have a specific look at the initialization that has been
performed via</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a><span class="fu">str</span>(bt0<span class="sc">$</span>BTInit)</span></code></pre></div>
<pre><code>## List of 3
##  $ initFit         : num 0.145
##  $ training.error  : num 0.365
##  $ validation.error: num 0.371
##  - attr(*, &quot;class&quot;)= chr &quot;BTInit&quot;</code></pre>
<p>If <code>keep.data=TRUE</code>, the different databases with the last
evaluation are returned</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" tabindex="-1"></a><span class="fu">str</span>(bt0<span class="sc">$</span>BTData)</span></code></pre></div>
<pre><code>## List of 2
##  $ training.set  :&#39;data.frame&#39;:  32000 obs. of  7 variables:
##   ..$ Y_normalized  : num [1:32000] 0 1.15 0 0 3.33 ...
##   ..$ Gender        : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 1 1 2 1 1 1 2 2 1 ...
##   ..$ Age           : int [1:32000] 53 22 22 21 59 64 49 31 61 28 ...
##   ..$ Split         : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 1 2 1 2 1 2 1 2 ...
##   ..$ Sport         : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 2 2 1 1 1 1 2 1 1 ...
##   ..$ w             : num [1:32000] 0.977 0.869 0.479 0.374 0.3 ...
##   ..$ currTrainScore: num [1:32000] -1.99 -1.86 -1.84 -1.84 -1.98 ...
##  $ validation.set:&#39;data.frame&#39;:  8000 obs. of  7 variables:
##   ..$ Y_normalized: num [1:8000] 0 0 0 0 0 0 0 0 0 0 ...
##   ..$ Gender      : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 2 2 2 1 1 1 2 1 1 ...
##   ..$ Age         : int [1:8000] 49 27 45 39 28 65 19 21 43 62 ...
##   ..$ Split       : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 2 2 2 1 1 1 2 1 2 ...
##   ..$ Sport       : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 2 1 2 2 2 1 1 2 ...
##   ..$ w           : num [1:8000] 0.645 0.175 0.625 0.402 0.572 ...
##   ..$ currValScore: num [1:8000] -1.98 -1.94 -1.94 -1.99 -1.93 ...
##  - attr(*, &quot;class&quot;)= chr &quot;BTData&quot;</code></pre>
<p>The fitted values (on the score scale) as well as the computed errors
across the iterations are available</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" tabindex="-1"></a><span class="fu">head</span>(bt0<span class="sc">$</span>fitted.values, <span class="dv">5</span>)</span></code></pre></div>
<pre><code>## [1] -1.991089 -1.856422 -1.838851 -1.843473 -1.980151</code></pre>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" tabindex="-1"></a><span class="fu">str</span>(bt0<span class="sc">$</span>BTErrors)</span></code></pre></div>
<pre><code>## List of 3
##  $ training.error  : num [1:50] 0.369 0.365 0.37 0.368 0.37 ...
##  $ validation.error: num [1:50] 0.371 0.371 0.371 0.371 0.371 ...
##  $ oob.improvement : num [1:50] 2.11e-05 1.09e-05 1.53e-05 5.85e-06 1.44e-05 ...
##  - attr(*, &quot;class&quot;)= chr &quot;BTErrors&quot;</code></pre>
<p>Finally, each weak learner (tree) built in the expansion are stored
within the following object. Each element corresponds to a specific
<code>rpart</code> object.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" tabindex="-1"></a><span class="fu">length</span>(bt0<span class="sc">$</span>BTIndivFits)</span></code></pre></div>
<pre><code>## [1] 50</code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" tabindex="-1"></a><span class="co"># First tree in the expansion.</span></span>
<span id="cb44-2"><a href="#cb44-2" tabindex="-1"></a>bt0<span class="sc">$</span>BTIndivFits[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>## n= 16000 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
## 1) root 16000 5903.5850 1.0208110  
##   2) Age&gt;=24.5 13674 4892.0440 0.9677506  
##     4) Gender=female 6730 2296.5900 0.8985216  
##       8) Sport=no 3374 1103.4690 0.8238464 *
##       9) Sport=yes 3356 1189.9800 0.9744537 *
##     5) Gender=male 6944 2590.6080 1.0350400 *
##   3) Age&lt; 24.5 2326  993.6583 1.3367930 *</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" tabindex="-1"></a>bt0<span class="sc">$</span>BTIndivFits[[<span class="dv">1</span>]]<span class="sc">$</span>frame</span></code></pre></div>
<pre><code>##      var     n        wt       dev      yval   complexity ncompete nsurrogate
## 1    Age 16000 1168.6784 5903.5849 1.0208112 0.0030290772        3          0
## 2 Gender 13674 1001.3448 4892.0442 0.9677506 0.0008209918        3          1
## 4  Sport  6730  493.1654 2296.5895 0.8985216 0.0005319459        2          2
## 8 &lt;leaf&gt;  3374  247.8532 1103.4687 0.8238464         -Inf        0          0
## 9 &lt;leaf&gt;  3356  245.3122 1189.9804 0.9744537         -Inf        0          0
## 5 &lt;leaf&gt;  6944  508.1794 2590.6078 1.0350402 0.0003220048        0          0
## 3 &lt;leaf&gt;  2326  167.3337  993.6583 1.3367928 0.0003038660        0          0
##        yval2.1      yval2.2
## 1    1.0208112 1193.0000000
## 2    0.9677506  969.0000000
## 4    0.8985216  443.0000000
## 8    0.8238464  204.0000000
## 9    0.9744537  239.0000000
## 5    1.0350402  526.0000000
## 3    1.3367928  224.0000000</code></pre>
</div>
<div id="optimal-iterations-number" class="section level3">
<h3>Optimal iterations number</h3>
<p><code>BT_perf</code> function allows the user to determine the best
number of iterations that has to be performed. This one also depends on
the type of errors that are available/have been computed during training
phase.</p>
<p>Depending on the chosen approach, the following methods can be
applied to compute the best number of iterations:</p>
<ul>
<li>If user wants to use the <code>validation.error</code>, the
<code>argmin(BT$BTErrors$validation.error)</code> will be returned as
optimal iteration.</li>
<li>If user wants to use the <code>oob.improvement</code>, the
<code>argmin(-cumsum(BT$BTErrors$oob.improvement))</code> will be
returned as optimal iteration. To be precise, the
<code>oob.improvement</code> are not used as such but a smoothed version
of it.</li>
<li>If user wants to use the <code>cv.error</code>, the
<code>argmin(BT$BTErrors$cv.error)</code> will be returned as optimal
iteration.</li>
</ul>
<p>We refer to the package documentation <code>?BT::BT_perf</code> for a
thorough presentation of this function arguments.</p>
<p>In our specific context, only the OOB improvements and validation
errors are available for the given run (no cross-validation
performed).</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" tabindex="-1"></a>perfbt0_OOB <span class="ot">&lt;-</span> <span class="fu">BT_perf</span>(bt0, <span class="at">method =</span> <span class="st">&quot;OOB&quot;</span>, <span class="at">oobag.curve =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## OOB generally underestimates the optimal number of iterations although predictive performance is reasonably competitive.
##             Using cv_folds&gt;1 when calling BT usually results in improved predictive performance.</code></pre>
<p><img src="data:image/png;base64,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" style="display: block; margin: auto;" /><img src="data:image/png;base64,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" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" tabindex="-1"></a>perfbt0_OOB</span></code></pre></div>
<pre><code>## [1] 50</code></pre>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" tabindex="-1"></a>perfbt0_val <span class="ot">&lt;-</span> <span class="fu">BT_perf</span>(bt0, <span class="at">method =</span> <span class="st">&quot;validation&quot;</span>)</span></code></pre></div>
<p><img src="data:image/png;base64,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" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" tabindex="-1"></a>perfbt0_val</span></code></pre></div>
<pre><code>## [1] 50</code></pre>
<p>Using the implemented “best guess” approach</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" tabindex="-1"></a>perfbt0_BG <span class="ot">&lt;-</span> <span class="fu">BT_perf</span>(bt0, <span class="at">plot.it =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## Using validation method...</code></pre>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" tabindex="-1"></a>perfbt0_BG</span></code></pre></div>
<pre><code>## [1] 50</code></pre>
</div>
<div id="continue-training" class="section level3">
<h3>Continue training</h3>
<p>It clearly seems that our model does not contain enough weak
learners. In fact, the optimal number of iterations is equal to the
model number of iterations, meaning that the minimal error (and the
related iteration) should still be found. It’s therefore interesting to
continue the training.</p>
<p>This training continuation can be performed thanks to the
<code>BT_more</code> function. The arguments of this function are
explained in <code>?BT::BT_more</code> and we therefore refer to it for
more details.</p>
<p>It will then return a <code>BTFit</code> object (as the
<code>BT</code> function does) augmented by the new boosting
iterations.</p>
<p>We emphasize that the call to this function can only be made if the
original <code>BT</code> call:</p>
<ul>
<li>has no cross-validation;</li>
<li>has been computed with <code>keep.data</code> parameter set to
<code>TRUE</code>.</li>
</ul>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" tabindex="-1"></a>bt1 <span class="ot">&lt;-</span> <span class="fu">BT_more</span>(bt0, <span class="at">new.n.iter =</span> <span class="dv">150</span>, <span class="at">seed =</span> <span class="dv">4</span>)</span>
<span id="cb59-2"><a href="#cb59-2" tabindex="-1"></a><span class="co"># See parameters and different inputs.</span></span>
<span id="cb59-3"><a href="#cb59-3" tabindex="-1"></a>bt1<span class="sc">$</span>BTParams<span class="sc">$</span>n.iter</span></code></pre></div>
<pre><code>## [1] 200</code></pre>
<p>It clearly seems that we now reached an optimum.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" tabindex="-1"></a>perfbt1_OOB <span class="ot">&lt;-</span> <span class="fu">BT_perf</span>(bt1, <span class="at">method =</span> <span class="st">&quot;OOB&quot;</span>, <span class="at">plot.it =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## OOB generally underestimates the optimal number of iterations although predictive performance is reasonably competitive.
##             Using cv_folds&gt;1 when calling BT usually results in improved predictive performance.</code></pre>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" tabindex="-1"></a>perfbt1_val <span class="ot">&lt;-</span> <span class="fu">BT_perf</span>(bt1, <span class="at">method =</span> <span class="st">&quot;validation&quot;</span>, <span class="at">plot.it =</span> <span class="cn">FALSE</span>)</span>
<span id="cb63-2"><a href="#cb63-2" tabindex="-1"></a>perfbt1_OOB</span></code></pre></div>
<pre><code>## [1] 80</code></pre>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" tabindex="-1"></a>perfbt1_val</span></code></pre></div>
<pre><code>## [1] 191</code></pre>
</div>
<div id="cross-validation" class="section level3">
<h3>Cross-validation</h3>
<p>We often favor doing cross-validation to find the optimal number of
iterations. That being said, this approach can be time-consuming and a
balance has to be found by the modeler.</p>
<p>Let’s see the results if a 3-folds cross-validation is performed.
Please note that the <code>train.fraction</code> is now set to 1 as
creating a validation set is less meaningful in the cross-validation
context.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" tabindex="-1"></a>bt2 <span class="ot">&lt;-</span> <span class="fu">BT</span>(<span class="at">formula =</span> formFreq, <span class="at">data =</span> trainSet, <span class="at">tweedie.power =</span> <span class="dv">1</span>, <span class="at">ABT =</span> <span class="cn">FALSE</span>, <span class="at">n.iter =</span> <span class="dv">200</span>,</span>
<span id="cb67-2"><a href="#cb67-2" tabindex="-1"></a>    <span class="at">train.fraction =</span> <span class="dv">1</span>, <span class="at">interaction.depth =</span> <span class="dv">3</span>, <span class="at">shrinkage =</span> <span class="fl">0.01</span>, <span class="at">bag.fraction =</span> <span class="fl">0.5</span>,</span>
<span id="cb67-3"><a href="#cb67-3" tabindex="-1"></a>    <span class="at">colsample.bytree =</span> <span class="cn">NULL</span>, <span class="at">keep.data =</span> <span class="cn">TRUE</span>, <span class="at">is.verbose =</span> <span class="cn">FALSE</span>, <span class="at">cv.folds =</span> <span class="dv">3</span>,</span>
<span id="cb67-4"><a href="#cb67-4" tabindex="-1"></a>    <span class="at">folds.id =</span> <span class="cn">NULL</span>, <span class="at">n.cores =</span> <span class="dv">1</span>, <span class="at">weights =</span> ExpoR, <span class="at">seed =</span> <span class="dv">4</span>)</span></code></pre></div>
<p>Different objects are now available within the new <code>BT</code>
results</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" tabindex="-1"></a>bt2<span class="sc">$</span>cv.folds</span></code></pre></div>
<pre><code>## [1] 3</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" tabindex="-1"></a><span class="fu">str</span>(bt2<span class="sc">$</span>folds)</span></code></pre></div>
<pre><code>##  int [1:40000] 3 3 3 3 3 2 1 2 3 2 ...</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" tabindex="-1"></a><span class="fu">str</span>(bt2<span class="sc">$</span>cv.fitted)</span></code></pre></div>
<pre><code>##  num [1:40000] -2.06 -1.82 -1.79 -1.68 -2.04 ...</code></pre>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" tabindex="-1"></a><span class="fu">str</span>(bt2<span class="sc">$</span>BTErrors)</span></code></pre></div>
<pre><code>## List of 4
##  $ training.error  : num [1:200] 0.369 0.371 0.363 0.361 0.355 ...
##  $ validation.error: NULL
##  $ oob.improvement : num [1:200] 1.88e-05 2.37e-05 2.79e-05 3.01e-05 3.72e-06 ...
##  $ cv.error        : num [1:200] 0.366 0.366 0.366 0.366 0.366 ...
##  - attr(*, &quot;class&quot;)= chr &quot;BTErrors&quot;</code></pre>
<p>One can also find the optimal number of iterations via</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" tabindex="-1"></a>perfbt2_cv <span class="ot">&lt;-</span> <span class="fu">BT_perf</span>(bt2, <span class="at">method =</span> <span class="st">&quot;cv&quot;</span>)</span></code></pre></div>
<p><img src="data:image/png;base64,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" style="display: block; margin: auto;" /></p>
</div>
<div id="hyperparameter-optimization" class="section level3">
<h3>Hyperparameter Optimization</h3>
<p>We only worked with one parameter set up to now. In practice, this
set has to be found. An usual approach consists in performing a grid
search and assessing the performances via cross-validation. Please note
that using a validation set can also be used, depending on the
computation time.</p>
<p>For this presentation, only one extra boosting tree algorithm will be
fitted. In particular, only one different value for
<code>interaction.depth</code> will be investigated. In reality the grid
search should be way broader, trying multiple multidimensional
combinations involving different parameters.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" tabindex="-1"></a>bt3 <span class="ot">&lt;-</span> <span class="fu">BT</span>(<span class="at">formula =</span> formFreq, <span class="at">data =</span> trainSet, <span class="at">tweedie.power =</span> <span class="dv">1</span>, <span class="at">ABT =</span> <span class="cn">FALSE</span>, <span class="at">n.iter =</span> <span class="dv">225</span>,</span>
<span id="cb77-2"><a href="#cb77-2" tabindex="-1"></a>    <span class="at">train.fraction =</span> <span class="dv">1</span>, <span class="at">interaction.depth =</span> <span class="dv">2</span>, <span class="at">shrinkage =</span> <span class="fl">0.01</span>, <span class="at">bag.fraction =</span> <span class="fl">0.5</span>,</span>
<span id="cb77-3"><a href="#cb77-3" tabindex="-1"></a>    <span class="at">colsample.bytree =</span> <span class="cn">NULL</span>, <span class="at">keep.data =</span> <span class="cn">TRUE</span>, <span class="at">is.verbose =</span> <span class="cn">FALSE</span>, <span class="at">cv.folds =</span> <span class="dv">3</span>,</span>
<span id="cb77-4"><a href="#cb77-4" tabindex="-1"></a>    <span class="at">folds.id =</span> <span class="cn">NULL</span>, <span class="at">n.cores =</span> <span class="dv">1</span>, <span class="at">weights =</span> ExpoR, <span class="at">seed =</span> <span class="dv">4</span>)</span></code></pre></div>
<p>We generally select the best model by finding the one with the lowest
cross-validation error.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" tabindex="-1"></a>indexMin <span class="ot">&lt;-</span> <span class="fu">which.min</span>(<span class="fu">c</span>(<span class="fu">min</span>(bt2<span class="sc">$</span>BTErrors<span class="sc">$</span>cv.error), <span class="fu">min</span>(bt3<span class="sc">$</span>BTErrors<span class="sc">$</span>cv.error)))</span>
<span id="cb78-2"><a href="#cb78-2" tabindex="-1"></a>btOpt <span class="ot">&lt;-</span> <span class="cf">if</span> (indexMin <span class="sc">==</span> <span class="dv">1</span>) bt2 <span class="cf">else</span> bt3</span>
<span id="cb78-3"><a href="#cb78-3" tabindex="-1"></a>perfbtOpt_cv <span class="ot">&lt;-</span> <span class="fu">BT_perf</span>(btOpt, <span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">plot.it =</span> <span class="cn">FALSE</span>)</span>
<span id="cb78-4"><a href="#cb78-4" tabindex="-1"></a></span>
<span id="cb78-5"><a href="#cb78-5" tabindex="-1"></a>btOpt</span></code></pre></div>
<pre><code>## BT(formula = formFreq, data = trainSet, tweedie.power = 1, ABT = FALSE, 
##     n.iter = 225, train.fraction = 1, interaction.depth = 2, 
##     shrinkage = 0.01, bag.fraction = 0.5, colsample.bytree = NULL, 
##     keep.data = TRUE, is.verbose = FALSE, cv.folds = 3, folds.id = NULL, 
##     n.cores = 1, weights = ExpoR, seed = 4)
## A boosting tree model with Tweedie parameter : 1  has been fitted.
##  225 iterations were performed.</code></pre>
<pre><code>## OOB generally underestimates the optimal number of iterations although predictive performance is reasonably competitive.
##             Using cv_folds&gt;1 when calling BT usually results in improved predictive performance.</code></pre>
<pre><code>## The best out-of-bag iteration was 113.
## The best cross-validation iteration was 199.
## There were 4 predictors of which 4 had non-zero influence.</code></pre>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1" tabindex="-1"></a>perfbtOpt_cv</span></code></pre></div>
<pre><code>## [1] 199</code></pre>
</div>
<div id="relative-influence" class="section level3">
<h3>Relative influence</h3>
<p>Now that the optimal model has been found, one can compute the
relative influence. It corresponds to the gain made by splitting over
the features, which is then normalized in order to sum up to 100%.</p>
<p>The <code>summary</code> function allows to compute these values and
plot. We refer to this function documentation
<code>?BT:::summary.BTFit</code> for a thorough presentation.</p>
<p>The computation of the relative influence isn’t currently available
for the permutation approach. This one should still be developed.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" tabindex="-1"></a><span class="fu">summary</span>(btOpt, <span class="at">n.iter =</span> perfbtOpt_cv)</span></code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<pre><code>##           var   rel_inf
## Age    Gender 71.815815
## Sport   Sport 17.717977
## Gender    Age  7.899891
## Split   Split  2.566317</code></pre>
</div>
<div id="prediction" class="section level3">
<h3>Prediction</h3>
<p>Fortunately, once a <code>BT</code> object created we can use it to
predict on a new database, using the <code>predict</code> function. To
this end, the optimal number of iterations is generally a desirable
input. We also underline that the model fitted on the whole training set
is used to perform these predictions.</p>
<p>The interested reader can find a description of the function
arguments in the related documentation <code>?BT:::predict.BTFit</code>.
Please note that if the <code>keep.data</code> argument was set to
<code>TRUE</code> and if the <code>newdata</code> is not specified, the
prediction will be achieved on the original training set.</p>
<p>We explicit below two usages:</p>
<ul>
<li>Prediction (on the link/response scale) using all weak learners up
to the best iteration obtained via OOB and CV (it provides 2-dimensional
matrix). This is one of the most common option used.</li>
</ul>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">predict</span>(btOpt, <span class="at">n.iter =</span> <span class="fu">c</span>(<span class="fu">BT_perf</span>(btOpt, <span class="at">method =</span> <span class="st">&quot;OOB&quot;</span>, <span class="at">plot.it =</span> <span class="cn">FALSE</span>), perfbtOpt_cv),</span>
<span id="cb86-2"><a href="#cb86-2" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">&quot;link&quot;</span>), <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## As newdata is missing or is not a data frame, the training set has been used thanks to the keep.data = TRUE parameter.</code></pre>
<pre><code>## OOB generally underestimates the optimal number of iterations although predictive performance is reasonably competitive.
##             Using cv_folds&gt;1 when calling BT usually results in improved predictive performance.</code></pre>
<pre><code>##            [,1]      [,2]
##  [1,] -2.036076 -2.073071
##  [2,] -1.795237 -1.778898
##  [3,] -1.791303 -1.765213
##  [4,] -1.723679 -1.685962
##  [5,] -2.030221 -2.062991
##  [6,] -2.030771 -2.066194
##  [7,] -2.030035 -2.058636
##  [8,] -1.900488 -1.881005
##  [9,] -2.006163 -2.020618
## [10,] -1.995247 -2.015617</code></pre>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="#cb90-1" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">predict</span>(btOpt, <span class="at">n.iter =</span> <span class="fu">c</span>(<span class="fu">BT_perf</span>(btOpt, <span class="at">method =</span> <span class="st">&quot;OOB&quot;</span>, <span class="at">plot.it =</span> <span class="cn">FALSE</span>), perfbtOpt_cv),</span>
<span id="cb90-2"><a href="#cb90-2" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">&quot;response&quot;</span>), <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## As newdata is missing or is not a data frame, the training set has been used thanks to the keep.data = TRUE parameter.
## OOB generally underestimates the optimal number of iterations although predictive performance is reasonably competitive.
##             Using cv_folds&gt;1 when calling BT usually results in improved predictive performance.</code></pre>
<pre><code>##            [,1]      [,2]
##  [1,] 0.1305400 0.1257988
##  [2,] 0.1660881 0.1688241
##  [3,] 0.1667428 0.1711503
##  [4,] 0.1784085 0.1852661
##  [5,] 0.1313065 0.1270734
##  [6,] 0.1312343 0.1266669
##  [7,] 0.1313309 0.1276280
##  [8,] 0.1494957 0.1524368
##  [9,] 0.1345038 0.1325735
## [10,] 0.1359800 0.1332382</code></pre>
<ul>
<li>Prediction (on the response scale) using only the 40th weak learner
(tree).</li>
</ul>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">predict</span>(btOpt, <span class="at">n.iter =</span> <span class="dv">40</span>, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>, <span class="at">single.iter =</span> <span class="cn">TRUE</span>), <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## As newdata is missing or is not a data frame, the training set has been used thanks to the keep.data = TRUE parameter.</code></pre>
<pre><code>##  [1] 0.9192015 0.9192015 0.9192015 1.2280911 0.9192015 0.9192015 0.9192015
##  [8] 1.0333582 1.0333582 0.9192015</code></pre>
</div>
</div>
<div id="adaptive-boosting-tree-abt" class="section level2">
<h2>Adaptive Boosting Tree (ABT)</h2>
<p>All the functions available on the classical Boosting Tree side are
also available in the Adaptive Boosting Tree context. The only
difference lies in the way the number of internal nodes is defined. For
a given <code>interaction.depth</code>, ABT will in fact look for the
biggest optimal tree having at most <code>interaction.depth</code>
internal nodes (i.e. the built weak learner). This idea is basically
based on the <code>rpart</code> complexity parameter. Differently said,
all the trees in the expansion won’t necessarily contain
<code>interaction.depth</code> internal nodes.</p>
<p>By construction, it’s interesting to note that the built trees will
converge to a single root node. This can therefore acts as a natural
stopping criteria helping to reduce the computation time. However, this
option is not implemented in the <code>BT</code> package. Moreover, this
behavior is not necessarily observed when random effects (e.g. bag
fraction) are used.</p>
<div id="hyperparameter-optimization-1" class="section level3">
<h3>Hyperparameter Optimization</h3>
<p>As we did in the BT side, we’ll test two parameters combination and
assess their performances via cross-validation. Precisely, values 2 and
3 will be tried out for the <code>interaction.depth</code> parameter.
Let’s start by defining the parameters grid thanks to the
<code>base::expand.grid</code> function. Once again, we acknowledge that
it corresponds to a small representation of real-world problems.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="#cb96-1" tabindex="-1"></a>nIterVec <span class="ot">&lt;-</span> <span class="dv">225</span></span>
<span id="cb96-2"><a href="#cb96-2" tabindex="-1"></a>interactionDepthVec <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb96-3"><a href="#cb96-3" tabindex="-1"></a>shrinkageVec <span class="ot">&lt;-</span> <span class="fl">0.01</span></span>
<span id="cb96-4"><a href="#cb96-4" tabindex="-1"></a>bagFractionVec <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb96-5"><a href="#cb96-5" tabindex="-1"></a></span>
<span id="cb96-6"><a href="#cb96-6" tabindex="-1"></a>gridSearch <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">n.iter =</span> nIterVec, <span class="at">interaction.depth =</span> interactionDepthVec,</span>
<span id="cb96-7"><a href="#cb96-7" tabindex="-1"></a>    <span class="at">shrinkage =</span> shrinkageVec, <span class="at">bag.fraction =</span> bagFractionVec)</span>
<span id="cb96-8"><a href="#cb96-8" tabindex="-1"></a>gridSearch</span></code></pre></div>
<pre><code>##   n.iter interaction.depth shrinkage bag.fraction
## 1    225                 2      0.01          0.5
## 2    225                 3      0.01          0.5</code></pre>
<p>We can now loop through all the different scenarios.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="#cb98-1" tabindex="-1"></a>abtRes_cv <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb98-2"><a href="#cb98-2" tabindex="-1"></a><span class="cf">for</span> (iGrid <span class="cf">in</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="fu">nrow</span>(gridSearch))) {</span>
<span id="cb98-3"><a href="#cb98-3" tabindex="-1"></a>    currABT <span class="ot">&lt;-</span> <span class="fu">BT</span>(<span class="at">formula =</span> formFreq, <span class="at">data =</span> trainSet, <span class="at">tweedie.power =</span> <span class="dv">1</span>, <span class="at">ABT =</span> <span class="cn">TRUE</span>,</span>
<span id="cb98-4"><a href="#cb98-4" tabindex="-1"></a>        <span class="at">n.iter =</span> gridSearch[iGrid, <span class="st">&quot;n.iter&quot;</span>], <span class="at">train.fraction =</span> <span class="dv">1</span>, <span class="at">interaction.depth =</span> gridSearch[iGrid,</span>
<span id="cb98-5"><a href="#cb98-5" tabindex="-1"></a>            <span class="st">&quot;interaction.depth&quot;</span>], <span class="at">shrinkage =</span> gridSearch[iGrid, <span class="st">&quot;shrinkage&quot;</span>], <span class="at">bag.fraction =</span> gridSearch[iGrid,</span>
<span id="cb98-6"><a href="#cb98-6" tabindex="-1"></a>            <span class="st">&quot;bag.fraction&quot;</span>], <span class="at">colsample.bytree =</span> <span class="cn">NULL</span>, <span class="at">keep.data =</span> <span class="cn">FALSE</span>, <span class="at">is.verbose =</span> <span class="cn">FALSE</span>,</span>
<span id="cb98-7"><a href="#cb98-7" tabindex="-1"></a>        <span class="at">cv.folds =</span> <span class="dv">3</span>, <span class="at">folds.id =</span> <span class="cn">NULL</span>, <span class="at">n.cores =</span> <span class="dv">1</span>, <span class="at">weights =</span> ExpoR, <span class="at">seed =</span> <span class="dv">4</span>)</span>
<span id="cb98-8"><a href="#cb98-8" tabindex="-1"></a></span>
<span id="cb98-9"><a href="#cb98-9" tabindex="-1"></a>    abtRes_cv[[iGrid]] <span class="ot">&lt;-</span> currABT</span>
<span id="cb98-10"><a href="#cb98-10" tabindex="-1"></a>}</span></code></pre></div>
<p>Check that we’ve enough iterations and define the best ABT model.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" tabindex="-1"></a>perfabt1_cv <span class="ot">&lt;-</span> <span class="fu">BT_perf</span>(abtRes_cv[[<span class="dv">1</span>]], <span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">plot.it =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="data:image/png;base64,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" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="#cb100-1" tabindex="-1"></a>perfabt2_cv <span class="ot">&lt;-</span> <span class="fu">BT_perf</span>(abtRes_cv[[<span class="dv">2</span>]], <span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">plot.it =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="data:image/png;base64,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" style="display: block; margin: auto;" /></p>
<p>We can finally define the best ABT model.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1" tabindex="-1"></a>indexMin <span class="ot">&lt;-</span> <span class="fu">which.min</span>(<span class="fu">c</span>(<span class="fu">min</span>(abtRes_cv[[<span class="dv">1</span>]]<span class="sc">$</span>BTErrors<span class="sc">$</span>cv.error), <span class="fu">min</span>(abtRes_cv[[<span class="dv">2</span>]]<span class="sc">$</span>BTErrors<span class="sc">$</span>cv.error)))</span>
<span id="cb101-2"><a href="#cb101-2" tabindex="-1"></a>abtOpt <span class="ot">&lt;-</span> <span class="cf">if</span> (indexMin <span class="sc">==</span> <span class="dv">1</span>) abtRes_cv[[<span class="dv">1</span>]] <span class="cf">else</span> abtRes_cv[[<span class="dv">2</span>]]</span>
<span id="cb101-3"><a href="#cb101-3" tabindex="-1"></a>perfabtOpt_cv <span class="ot">&lt;-</span> <span class="cf">if</span> (indexMin <span class="sc">==</span> <span class="dv">1</span>) perfabt1_cv <span class="cf">else</span> perfabt2_cv</span>
<span id="cb101-4"><a href="#cb101-4" tabindex="-1"></a></span>
<span id="cb101-5"><a href="#cb101-5" tabindex="-1"></a>abtOpt</span></code></pre></div>
<pre><code>## BT(formula = formFreq, data = trainSet, tweedie.power = 1, ABT = TRUE, 
##     n.iter = gridSearch[iGrid, &quot;n.iter&quot;], train.fraction = 1, 
##     interaction.depth = gridSearch[iGrid, &quot;interaction.depth&quot;], 
##     shrinkage = gridSearch[iGrid, &quot;shrinkage&quot;], bag.fraction = gridSearch[iGrid, 
##         &quot;bag.fraction&quot;], colsample.bytree = NULL, keep.data = FALSE, 
##     is.verbose = FALSE, cv.folds = 3, folds.id = NULL, n.cores = 1, 
##     weights = ExpoR, seed = 4)
## An adaptive boosting tree model with Tweedie parameter : 1  has been fitted.
##  225 iterations were performed.</code></pre>
<pre><code>## OOB generally underestimates the optimal number of iterations although predictive performance is reasonably competitive.
##             Using cv_folds&gt;1 when calling BT usually results in improved predictive performance.</code></pre>
<pre><code>## The best out-of-bag iteration was 113.
## The best cross-validation iteration was 199.
## There were 4 predictors of which 4 had non-zero influence.</code></pre>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" tabindex="-1"></a>abtOpt<span class="sc">$</span>BTParams<span class="sc">$</span>interaction.depth</span></code></pre></div>
<pre><code>## [1] 2</code></pre>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" tabindex="-1"></a>perfabtOpt_cv</span></code></pre></div>
<pre><code>## [1] 199</code></pre>
</div>
</div>
<div id="miscellaneous" class="section level2">
<h2>Miscellaneous</h2>
<p>Let’s have a look at the resulting weak learners (trees) from BT and
ABT expansions.</p>
<p>In the BT case, all the trees contain exactly
<code>interaction.depth</code> internal nodes (or splits) whereas in the
ABT case one can notice the variation in number of internal nodes (and
so the trees’ shapes).</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1" tabindex="-1"></a><span class="fu">table</span>(<span class="fu">sapply</span>(<span class="fu">seq</span>(<span class="dv">1</span>, perfbtOpt_cv), <span class="cf">function</span>(xx) {</span>
<span id="cb109-2"><a href="#cb109-2" tabindex="-1"></a>    <span class="fu">nrow</span>(btOpt<span class="sc">$</span>BTIndivFits[[xx]]<span class="sc">$</span>frame[btOpt<span class="sc">$</span>BTIndivFits[[xx]]<span class="sc">$</span>frame<span class="sc">$</span>var <span class="sc">!=</span> <span class="st">&quot;&lt;leaf&gt;&quot;</span>,</span>
<span id="cb109-3"><a href="#cb109-3" tabindex="-1"></a>        ])</span>
<span id="cb109-4"><a href="#cb109-4" tabindex="-1"></a>}))</span></code></pre></div>
<pre><code>## 
##   2 
## 199</code></pre>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="#cb111-1" tabindex="-1"></a><span class="fu">table</span>(<span class="fu">sapply</span>(<span class="fu">seq</span>(<span class="dv">1</span>, perfabtOpt_cv), <span class="cf">function</span>(xx) {</span>
<span id="cb111-2"><a href="#cb111-2" tabindex="-1"></a>    <span class="fu">nrow</span>(abtOpt<span class="sc">$</span>BTIndivFits[[xx]]<span class="sc">$</span>frame[abtOpt<span class="sc">$</span>BTIndivFits[[xx]]<span class="sc">$</span>frame<span class="sc">$</span>var <span class="sc">!=</span> <span class="st">&quot;&lt;leaf&gt;&quot;</span>,</span>
<span id="cb111-3"><a href="#cb111-3" tabindex="-1"></a>        ])</span>
<span id="cb111-4"><a href="#cb111-4" tabindex="-1"></a>}))</span></code></pre></div>
<pre><code>## 
##   0   2 
##   4 195</code></pre>
</div>
<div id="models-comparison" class="section level2">
<h2>Models comparison</h2>
<p>Once the optimal competing models have been defined, one can assess
their generalization performances (i.e. on the test set). To do so,
multiple criteria might be used, such as deviance, lift curves,
concordance measures, …</p>
<p>Only the first criterion will be investigated for this
presentation.</p>
<p><strong>Please note that usually only 1 model is retained beforehand
- The test set is not used for model selection. Our specific example
remains a case-study!</strong></p>
<p>Let’s start by computing the different model predictions on the test
set.</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="#cb113-1" tabindex="-1"></a>btPredTest <span class="ot">&lt;-</span> <span class="fu">predict</span>(btOpt, <span class="at">newdata =</span> testSet, <span class="at">n.iter =</span> perfbtOpt_cv, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">*</span></span>
<span id="cb113-2"><a href="#cb113-2" tabindex="-1"></a>    testSet<span class="sc">$</span>ExpoR</span>
<span id="cb113-3"><a href="#cb113-3" tabindex="-1"></a>abtPredTest <span class="ot">&lt;-</span> <span class="fu">predict</span>(abtOpt, <span class="at">newdata =</span> testSet, <span class="at">n.iter =</span> perfabtOpt_cv, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">*</span></span>
<span id="cb113-4"><a href="#cb113-4" tabindex="-1"></a>    testSet<span class="sc">$</span>ExpoR</span></code></pre></div>
<div id="deviance" class="section level3">
<h3>Deviance</h3>
<p>The deviance is defined as 2 times the log-likelihood ratio of the
saturated model compared to the reduced (fitted) one. In other words, it
measures the gap between the optimal model and the current one.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="#cb114-1" tabindex="-1"></a>devPoisson <span class="ot">&lt;-</span> <span class="cf">function</span>(obs, pred) {</span>
<span id="cb114-2"><a href="#cb114-2" tabindex="-1"></a>    <span class="dv">2</span> <span class="sc">*</span> (<span class="fu">sum</span>(<span class="fu">dpois</span>(<span class="at">x =</span> obs, <span class="at">lambda =</span> obs, <span class="at">log =</span> <span class="cn">TRUE</span>)) <span class="sc">-</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(<span class="at">x =</span> obs, <span class="at">lambda =</span> pred,</span>
<span id="cb114-3"><a href="#cb114-3" tabindex="-1"></a>        <span class="at">log =</span> <span class="cn">TRUE</span>)))</span>
<span id="cb114-4"><a href="#cb114-4" tabindex="-1"></a>}</span></code></pre></div>
<p>One can now assess the deviance of the different models.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1" tabindex="-1"></a><span class="fu">devPoisson</span>(testSet<span class="sc">$</span>Y, btPredTest)</span></code></pre></div>
<pre><code>## [1] 3618.013</code></pre>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="#cb117-1" tabindex="-1"></a><span class="fu">devPoisson</span>(testSet<span class="sc">$</span>Y, abtPredTest)</span></code></pre></div>
<pre><code>## [1] 3618.06</code></pre>
<p>For this simulated use-case, it therefore seems that the usual
boosting tree approach performs better.</p>
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
